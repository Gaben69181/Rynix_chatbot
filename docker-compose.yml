version: "3.9"

services:
  app:
    build: .
    container_name: openlm-chat
    ports:
      - "8501:8501"
    environment:
      # Streamlit
      - STREAMLIT_SERVER_HEADLESS=true
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
      # Provider endpoints / keys (optional; choose via sidebar in app)
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      # Persist HF cache across runs
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface
    volumes:
      - hf-cache:/root/.cache/huggingface
    depends_on:
      - ollama
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    restart: unless-stopped

volumes:
  hf-cache:
  ollama: