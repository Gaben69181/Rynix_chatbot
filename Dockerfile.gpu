FROM pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime

ENV PIP_NO_CACHE_DIR=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    STREAMLIT_SERVER_HEADLESS=true \
    STREAMLIT_SERVER_PORT=8501 \
    STREAMLIT_BROWSER_GATHER_USAGE_STATS=false

WORKDIR /app

# Keep the GPU-enabled torch from the base image. The requirements constraint (torch>=2.2.0)
# will be satisfied by the preinstalled CUDA build.
COPY requirements.txt /app/requirements.txt
RUN pip install --upgrade pip \
    && pip install -r /app/requirements.txt

COPY . /app

EXPOSE 8501

# Default Ollama endpoint for container-to-host access; override as needed
ENV OLLAMA_BASE_URL=http://host.docker.internal:11434

CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]